#First create a DB with schema
create database customer_data
create schema raw 
create schema updated_data

#Now create 2 tables to hold the raw and history data
create or replace TABLE customer_data.raw.CUSTOMER_RAW (
	ID NUMBER(38,0),
	CUSTOMER_NAME VARCHAR(16777216),
	PHONE_NUMBER VARCHAR(16777216),
	EMAIL VARCHAR(16777216),
	ADDRESS VARCHAR(16777216),
	STATE VARCHAR(16777216),
	OCCUPATION VARCHAR(16777216)
);


create or replace TABLE customer_data.updated_data.CUSTOMER_LATEST (
	ID NUMBER(38,0),
	CUSTOMER_NAME VARCHAR(16777216),
	PHONE_NUMBER VARCHAR(16777216),
	EMAIL VARCHAR(16777216),
	ADDRESS VARCHAR(16777216),
	STATE VARCHAR(16777216),
	OCCUPATION VARCHAR(16777216),
	UPDATED_TIME TIMESTAMP_LTZ(9) default current_timestamp()
);

#Once the tables are created now create a storage integration to S3 
create or replace storage integration streaming
    type = external_stage
    storage_provider = s3
    storage_aws_role_arn = 'arn:aws:iam::069311595498:role/streaming_to_snowflake'
    enabled = true
    storage_allowed_locations = ( 's3://nifi-pull-extract-data','s3://testing-snowflake-8' )
    -- storage_blocked_locations = ( 's3://<location1>', 's3://<location2>' )
    comment = 'This storage integration is used to connect s3 and snowflake'
    ;

desc storage integration streaming

#Once the storage integartion is successfully established wiith snowflake create a stage along with file format as well

create or replace stage streaming_stage
url='s3://testing-snowflake-8'
storage_integration=streaming

create or replace file format streaming_file_check
type=CSV
field_delimiter=','
skip_header=1
field_optionally_enclosed_by='"'

# Before creating the snowpipe test it using this command
copy into CUSTOMER_DATA.RAW.CUSTOMER_RAW
from @streaming_stage
file_format=(format_name='streaming_file_check')

#This is the snow pipe which will be loading data to raw table
create or replace pipe stream_pipe
auto_ingest=TRUE
as 
copy into CUSTOMER_DATA.RAW.CUSTOMER_RAW
from @streaming_stage
file_format=(format_name='streaming_file_check')


#Once every thing is tested wrt to the data check in the raw layer. Now, we need to create a stream to capture CDC
create or replace stream customer_changes
on table CUSTOMER_DATA.RAW.CUSTOMER_RAW

#Finally we will creating tasks to automate the data load and removal of old raw data in the layer 
create or replace task stream_task
warehouse='compute_wh'
as
merge into CUSTOMER_DATA.UPDATED_DATA.CUSTOMER_LATEST l
using CUSTOMER_DATA.RAW.CUSTOMER_CHANGES r
on l.id=r.id
when matched 
and l.customer_name<>r.customer_name
then update set
l.customer_name=r.customer_name,
l.updated_time=current_timestamp()
when not matched then insert (id,customer_name,phone_number,email,address,state,occupation,updated_time)
values (r.id,r.customer_name,r.phone_number,r.email,r.address,r.state,r.occupation,current_timestamp())

create or replace task removal_of_source_data
warehouse='compute_wh'
after stream_task
as
truncate table CUSTOMER_DATA.RAW.CUSTOMER_RAW
